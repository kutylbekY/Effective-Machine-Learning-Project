{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dirname, _, filenames in os.walk('input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_dir = \"input/train/train2500/\"\n",
    "# test_data_dir = \"input/test/test1250/\"\n",
    "train_data_dir = \"input_large/train/\"\n",
    "test_data_dir = \"input_large/test1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_cat_dog(Dataset):\n",
    "    def __init__(self, train_dir, transform=None):\n",
    "        self.train_dir = train_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(train_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.train_dir, self.images[index])\n",
    "        label = self.images[index].split(\".\")[0]\n",
    "        label = 0 if label == 'cat' else 1\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Resize((256,256), antialias=None)\n",
    "])\n",
    "val_size = 0.2 \n",
    "train_data = dataset_cat_dog(train_data_dir, transform)\n",
    "\n",
    "val_size = int(val_size * len(train_data))\n",
    "train_size = len(train_data) - val_size\n",
    "train_data, val_data = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# train_data = dataset_cat_dog(train_data_dir, transform)\n",
    "# val_data = dataset_cat_dog(test_data_dir, transform)\n",
    "\n",
    "# train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# val_dl = DataLoader(val_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 20000\n",
      "Validation Data Size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Size of the datasets\n",
    "print(\"Train Data Size:\", len(train_data))\n",
    "print(\"Validation Data Size:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, label = train_data[0]\n",
    "# print(img.shape,label)\n",
    "\n",
    "# def PrintImages():\n",
    "#     train_data = CatDogDataset(train_data_dir, transform)\n",
    "#     c = 0\n",
    "#     d = 0\n",
    "\n",
    "#     # Iterate over the dataset to see the images and labels\n",
    "#     for i in range(len(train_data)):\n",
    "#         image, label = train_data[i]\n",
    "\n",
    "#         # Print the label\n",
    "#         if label == 0 and c <= 10:\n",
    "#             c += 1\n",
    "#             print(\"Label: Cat - \", c)\n",
    "#         elif label == 1 and d <= 10:\n",
    "#             d += 1\n",
    "#             print(\"Label: Dog - \", d)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         # Transpose the image dimensions from (3, 150, 150) to (150, 150, 3)\n",
    "#         image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "#         # Display the image\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "#         print(\"-----------------------\")\n",
    "\n",
    "# PrintImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Images in tarining data : {len(train_dataset)}\")\n",
    "# print(f\"Images in test data : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Follwing classes are there : \\n\",train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.utils import make_grid\n",
    "\n",
    "# def show_batch(dl):\n",
    "#     for images, labels in dl:\n",
    "#         fig,ax = plt.subplots(figsize = (16,12))\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "#         ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "#         break\n",
    "\n",
    "# show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the CNN architecture\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         # self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu3(x)\n",
    "#         # x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        # self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # x = x.view(-1, 64 * 64 * 64)  # Flatten the output from convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=65536, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Loss: 0.6445\n",
      "Train Acc: 0.6227\n",
      "Val Loss: 0.5871\n",
      "Val Acc: 0.6998\n",
      "--------------------------------------\n",
      "Epoch 2/15\n",
      "Train Loss: 0.5653\n",
      "Train Acc: 0.7081\n",
      "Val Loss: 0.5357\n",
      "Val Acc: 0.7320\n",
      "--------------------------------------\n",
      "Epoch 3/15\n",
      "Train Loss: 0.5283\n",
      "Train Acc: 0.7313\n",
      "Val Loss: 0.5091\n",
      "Val Acc: 0.7530\n",
      "--------------------------------------\n",
      "Epoch 4/15\n",
      "Train Loss: 0.5023\n",
      "Train Acc: 0.7530\n",
      "Val Loss: 0.5422\n",
      "Val Acc: 0.7124\n",
      "--------------------------------------\n",
      "Epoch 5/15\n",
      "Train Loss: 0.4769\n",
      "Train Acc: 0.7730\n",
      "Val Loss: 0.4982\n",
      "Val Acc: 0.7544\n",
      "--------------------------------------\n",
      "Epoch 6/15\n",
      "Train Loss: 0.4543\n",
      "Train Acc: 0.7858\n",
      "Val Loss: 0.4905\n",
      "Val Acc: 0.7598\n",
      "--------------------------------------\n",
      "Epoch 7/15\n",
      "Train Loss: 0.4334\n",
      "Train Acc: 0.8000\n",
      "Val Loss: 0.4585\n",
      "Val Acc: 0.7830\n",
      "--------------------------------------\n",
      "Epoch 8/15\n",
      "Train Loss: 0.4132\n",
      "Train Acc: 0.8101\n",
      "Val Loss: 0.4570\n",
      "Val Acc: 0.7802\n",
      "--------------------------------------\n",
      "Epoch 9/15\n",
      "Train Loss: 0.3950\n",
      "Train Acc: 0.8222\n",
      "Val Loss: 0.4498\n",
      "Val Acc: 0.7880\n",
      "--------------------------------------\n",
      "Epoch 10/15\n",
      "Train Loss: 0.3771\n",
      "Train Acc: 0.8324\n",
      "Val Loss: 0.4409\n",
      "Val Acc: 0.7924\n",
      "--------------------------------------\n",
      "Epoch 11/15\n",
      "Train Loss: 0.3592\n",
      "Train Acc: 0.8424\n",
      "Val Loss: 0.4567\n",
      "Val Acc: 0.7852\n",
      "--------------------------------------\n",
      "Epoch 12/15\n",
      "Train Loss: 0.3452\n",
      "Train Acc: 0.8482\n",
      "Val Loss: 0.4534\n",
      "Val Acc: 0.7922\n",
      "--------------------------------------\n",
      "Epoch 13/15\n",
      "Train Loss: 0.3245\n",
      "Train Acc: 0.8605\n",
      "Val Loss: 0.4361\n",
      "Val Acc: 0.7990\n",
      "--------------------------------------\n",
      "Epoch 14/15\n",
      "Train Loss: 0.3072\n",
      "Train Acc: 0.8708\n",
      "Val Loss: 0.4446\n",
      "Val Acc: 0.7984\n",
      "--------------------------------------\n",
      "Epoch 15/15\n",
      "Train Loss: 0.2874\n",
      "Train Acc: 0.8783\n",
      "Val Loss: 0.4407\n",
      "Val Acc: 0.8050\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for images, labels in train_dl:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = train_loss / len(train_data)\n",
    "    train_acc = train_correct.double() / len(train_data)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dl:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = val_loss / len(val_data)\n",
    "    val_acc = val_correct.double() / len(val_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Acc: {val_acc:.4f}\")\n",
    "    print(\"--------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cat_dog_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: cat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"cat_dog_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation for the new images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load and preprocess a new image\n",
    "image_path = \"input_large/test1/200.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# Make a prediction on the new image\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "# Map the predicted label to the corresponding class name\n",
    "class_names = [\"cat\", \"dog\"]\n",
    "predicted_class = class_names[predicted.item()]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
